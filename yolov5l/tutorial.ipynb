{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9f7373-53c6-47e4-b306-8a9f7653abb1",
   "metadata": {},
   "source": [
    "This YOLOv5 ðŸš€ notebook by Vitis AI presents simple train, validate and predict examples to help start your AI adventure.\n",
    "We hope that the resources in this notebook will help you get the most out of YOLOv5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f241107",
   "metadata": {},
   "source": [
    "# Setup and Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d6162-9ebd-4b22-a501-a6d069c3d4cc",
   "metadata": {},
   "source": [
    "Clone GitHub repository, install dependencies and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d86a16c-3742-4e67-807c-bbd43e5f383c",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89f5d64-e171-48b6-8ae1-895d3c28edde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://xcdpython.xilinx.com/simple\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /envs/base/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (3.5.2)\n",
      "Requirement already satisfied: numpy<=1.21.5,>=1.18.5 in /envs/base/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.21.5)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2b49c-4a87-46dd-9f70-c1882c45098e",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141f90a-2b7f-4d1f-a11e-1e288137c7b0",
   "metadata": {},
   "source": [
    "##### Download COCO2017 dataset.(refer to this repo https://github.com/ultralytics/yolov5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de1d0c0-2c67-4533-82dd-772f1478055a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip  ...\n",
      "Downloading http://images.cocodataset.org/zips/train2017.zip ...\n",
      "Downloading http://images.cocodataset.org/zips/val2017.zip ...\n"
     ]
    }
   ],
   "source": [
    "!bash code/data/scripts/get_coco.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf86ac7-7152-4574-a819-8d6f719948e1",
   "metadata": {},
   "source": [
    "### Put coco2017 dataset under the ./code/data directory, dataset directory structure like:\n",
    "```markdown\n",
    "+ data/\n",
    "    + coco/\n",
    "        + labels/\n",
    "        + annotations/\n",
    "        + images/\n",
    "        + test-dev2017.txt \n",
    "        + train2017.txt\n",
    "        + val2017.txt\n",
    "```\n",
    "### Modify the path of coco in coco.yaml to your custom dataset\n",
    "```\n",
    "path: /path/to/the/coco_dataset  # dataset root dir\n",
    "train: train2017.txt\n",
    "val: val2017.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad3383-46c9-4c19-a6ac-4f163f41b80d",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e19ae-3f65-4d82-95f0-ec3e91b9782b",
   "metadata": {},
   "source": [
    "detect.py runs YOLOv5 inference on a variety of sources, downloading models automatically from the latest YOLOv5 release, and saving results to runs/detect. Example inference sources are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66895eed-8b86-4b02-9e9a-6bced791354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./../float/yolov5l.pt'], source=./../demo.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, nndct_quant=False\n",
      "YOLOv5 ðŸš€ 5248422 torch 1.8.0 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 539 layers, 46563709 parameters, 46563709 gradients, 109.6 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 438 layers, 46533693 parameters, 457725 gradients, 109.0 GFLOPs\n",
      "Speed: 0.4ms pre-process, 21.6ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd code\n",
    "!python detect.py --weights ./../float/yolov5l.pt --img 640 --conf 0.25 --source ./../demo.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92217f9f-a36b-4045-9f7f-9c5b3effac85",
   "metadata": {},
   "source": [
    "![avatar](./example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a69f4-271e-4759-a412-f6b22f580cc1",
   "metadata": {},
   "source": [
    "# Train/Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a4d38-906f-4df5-af62-ba9674242dc6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07455fb0-f2fa-433a-874c-f132c30c11cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=./../float/yolov5l.pt, cfg=models/yolov5l.yaml, data=data/coco.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=1, batch_size=2, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, adamw=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, nndct_quant=False, nndct_convert_sigmoid_to_hsigmoid=False, nndct_convert_silu_to_hswish=False, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5 ðŸš€ bd1c9652f torch 1.8.0 CUDA:0 (Tesla V100-PCIE-32GB, 32510.5MB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 539 layers, 46563709 parameters, 46563709 gradients, 109.6 GFLOPs\n",
      "\n",
      "Transferred 613/613 items from ./../float/yolov5l.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 101 weight, 104 weight (no decay), 104 bias\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.45, Best Possible Recall (BPR) = 0.9948\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "  0%|                                                 | 0/59144 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# for float training\n",
    "%cd code\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 train.py --data data/coco.yaml --cfg models/yolov5l.yaml --batch-size 2 --epochs 1 --weights ./../float/yolov5l.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230681a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb6b3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/coco.yaml, weights=['./../float/yolov5l.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=7, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, quant_mode=calib, nndct_quant=False, dump_xmodel=False, dump_onnx=False, with_postprocess=False, nndct_stat=0\n",
      "YOLOv5 ðŸš€ bd1c9652f torch 1.8.0 CUDA:7 (Tesla V100-PCIE-32GB, 32510.5MB)\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 539 layers, 46563709 parameters, 46563709 gradients, 109.6 GFLOPs\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 438 layers, 46533693 parameters, 46533693 gradients, 109.0 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335      0.749      0.604      0.668      0.479\n",
      "Speed: 0.1ms pre-process, 7.2ms inference, 1.3ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5l_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=5.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=61.39s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=11.99s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.673\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.320\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.545\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.624\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.372\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.616\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.508\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.727\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.813\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd code\n",
    "!CUDA_VISIBLE_DEVICES=0 python val.py --data data/coco.yaml --img 640 --conf 0.001 --iou 0.65 --weights ./../float/yolov5l.pt --device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b235516",
   "metadata": {},
   "source": [
    "## Quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c92fefb6-598a-438a-866f-2cb9b374788d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/coco.yaml, weights=['./../float/yolov5l.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=7, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, quant_mode=calib, nndct_quant=True, dump_xmodel=False, dump_onnx=False, with_postprocess=False, nndct_stat=0\n",
      "YOLOv5 ðŸš€ bd1c9652f torch 1.8.0 CUDA:7 (Tesla V100-PCIE-32GB, 32510.5MB)\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 539 layers, 46563709 parameters, 46563709 gradients, 109.6 GFLOPs\n",
      "\n",
      "NNDCT quant dir: ../float/nndct_quant\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "              release --- 4.15.0-166-generic\n",
      "              version --- #174-Ubuntu SMP Wed Dec 8 19:07:44 UTC 2021\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 9.4.0\n",
      "               python --- 3.7.12\n",
      "              pytorch --- 1.8.0\n",
      "        vai_q_pytorch --- 3.0.0+56fb10e+torch1.8.0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- Tesla V100-PCIE-32GB\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_REPLACE_SILU]: SiLU has been replaced by Hardswish.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model Model is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [00:00<00:00, 1689.0\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(../float/nndct_quant/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(../float/nndct_quant/quant_info.json)\u001b[0m\n",
      "                 all       5000      36335      0.708      0.567      0.624      0.391\n",
      "Speed: 0.1ms pre-process, 71.8ms inference, 1.1ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5l_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=7.14s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=63.69s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=17.26s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.629\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.454\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.245\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.462\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.530\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.324\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.607\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.657\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.753\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/coco.yaml, weights=['./../float/yolov5l.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=7, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, quant_mode=test, nndct_quant=True, dump_xmodel=False, dump_onnx=False, with_postprocess=False, nndct_stat=0\n",
      "YOLOv5 ðŸš€ bd1c9652f torch 1.8.0 CUDA:7 (Tesla V100-PCIE-32GB, 32510.5MB)\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 24      [17, 20, 23]  1    457725  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
      "Model Summary: 539 layers, 46563709 parameters, 46563709 gradients, 109.6 GFLOPs\n",
      "\n",
      "NNDCT quant dir: ../float/nndct_quant\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: OS and CPU information:\n",
      "               system --- Linux\n",
      "              release --- 4.15.0-166-generic\n",
      "              version --- #174-Ubuntu SMP Wed Dec 8 19:07:44 UTC 2021\n",
      "              machine --- x86_64\n",
      "            processor --- x86_64\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Tools version information:\n",
      "                  GCC --- GCC 9.4.0\n",
      "               python --- 3.7.12\n",
      "              pytorch --- 1.8.0\n",
      "        vai_q_pytorch --- 3.0.0+56fb10e+torch1.8.0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: GPU information:\n",
      "          device name --- Tesla V100-PCIE-32GB\n",
      "     device available --- True\n",
      "         device count --- 1\n",
      "       current device --- 0\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quant config file is empty, use default quant configuration\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cuda'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing Model...\u001b[0m\n",
      "\n",
      "\u001b[0;33m[VAIQ_WARN][QUANTIZER_TORCH_REPLACE_SILU]: SiLU has been replaced by Hardswish.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Start to trace and freeze model...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: The input model Model is torch.nn.Module.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Finish tracing.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Processing ops...\u001b[0m\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [00:00<00:00, 1523.8\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(../float/nndct_quant/Model.py)\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335      0.705      0.568      0.624      0.387\n",
      "Speed: 0.1ms pre-process, 30.9ms inference, 1.1ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp/yolov5l_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.94s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=6.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=69.54s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=15.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.401\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.628\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.450\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.244\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.460\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.527\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.322\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.547\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.448\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.750\n",
      "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd code\n",
    "!CUDA_VISIBLE_DEVICES=7 python val.py --data data/coco.yaml --img 640 --conf 0.001 --iou 0.65 --weights ./../float/yolov5l.pt --device 7 --nndct_quant --quant_mode calib \\\n",
    "                                            --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=7 python val.py --data data/coco.yaml --img 640 --conf 0.001 --iou 0.65 --weights ./../float/yolov5l.pt --device 7 --nndct_quant --quant_mode test \\\n",
    "                                            --nndct_convert_sigmoid_to_hsigmoid --nndct_convert_silu_to_hswish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34f8af-a6b2-4c5d-96ac-7f6ef206f514",
   "metadata": {},
   "source": [
    "## Performance\n",
    "| Model             | Input Size | Float mAP   | Quant mAP   | FLOPs  |\n",
    "|-------------------|------------|-------------|-------------|--------|\n",
    "| YOLOv5l           | 640\\*640   | 49.0%       | 45.7%       | 78.9G  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
